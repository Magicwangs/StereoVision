## MNIST
手写数字识别   
tensorflow例程学习
`tensor`：张量，n维阵列

## Deep Learning
Deep Learning强大的地方就是可以利用网络中间某一层的输出当做是数据的另一种表达，从而可以将其认为是经过网络学习到的特征，基于该特征，可以进行进一步的相似度比较等。
图像处理要想练成神经网络大法，必先减少参数加快速度
卷积神经网络有两种神器可以降低参数数目：局部感知野和权值共享

## 简单例程
## MNIST数据集组成
mnist.train
mnist.test
mnist.validation
图片大小28*28=784，平铺得到784长度的向量，在进阶中则保持2D结构  

图片 mnist.train.images：[55000, 784]，由0，1组成
标签 mnist.train.labels：[55000, 10]，是one-hot vectors
`one-hot vectors`：多数维度为0，某个维度为1

## softmax 回归
返回每个数字的可能性，softmax常用于分类
两个步骤：统计证据，转换为概率
`统计证据`：像素强度加权求和，对成为某个类有利的，权重为正，否则为负
每个数字的权重分配情况不同
`bias`：额外的证据，偏置量
两者之和为最终证据，然后再通过softmax回归，softmax就是对输入求幂，然后将其归一化

## 实现回归模型
Tensorflow不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python之外运行
`Variable`:一个可修改的张量

通过代价或损失来评估模型是好是坏的指标，最小化这个指标

`cross-entropy`:交叉熵，来计算损失
`学习率`：一点点的向目标靠近，太大会跳过最优点，太小学习太慢
`stochastic training`:随机训练，每次用一小部分的随机数据训练
`tf.argmax`:某个tensor对象在某一维上的其数据最大值所在的索引值。
`stride size`:步长
`padding size`:边距

## 进阶
详见代码
密集连接，就是全连接
ReLU操作（即一种激活函数），这个CNNs组成成员很小但很重要。其背后的数学原理也很简单：当一个输入值为负数，经过ReLU函数后输出为0

dropout：为了防止过拟合。
在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。
